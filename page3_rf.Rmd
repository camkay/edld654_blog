---
title: "Tensor Flo Ridas: A Journey in ML"
output: 
  html_document: 
    toc: TRUE
    toc_depth: 1
    toc_float: TRUE
    number_sections: TRUE
    code_folding: hide
    df_print: paged
---

```{r setup, include = FALSE}
# set chunk options
knitr::opts_chunk$set(message = FALSE,
                      warning = FALSE,
                      cache   = FALSE)

# load packages
library(tidyverse)
library(tidymodels)
library(baguette)
library(future)
library(vip)
library(rpart.plot)
library(here)
library(rio)
library(magrittr)

# create theme
theme_407 <- function() { 
    theme_bw(base_size = 5) %+replace% 
        theme(
          # legend.direction = "horizontal",
          legend.background = element_rect(fill = "#1b1d22",
                                           colour = "#1b1d22"),
          panel.background = element_rect(fill = "#171f24",
                              colour = "#1b1d22",
                              size = 0.5, linetype = "solid"),
          panel.grid.major = element_line(size = 0.2, 
                                          linetype = 'solid', 
                                          colour = "gray40"), 
          panel.grid.minor = element_line(size = 0.0, 
                                          linetype = 'solid',
                                          colour = "gray40"),
          axis.line = element_line(colour = "white"),
          plot.background = element_rect(fill = "#171f24"),
          text  = element_text(color = "white", size = 10),
          axis.text  = element_text(color = "white"),
          axis.ticks = element_line(color = "white"),
          strip.background = element_rect(fill = "white",
                                          colour = "white")
        )
}

# import all data
set.seed(3000)

data <- import(here::here("data", "data.csv"), setclass = "tibble") %>%
  janitor::clean_names() %>%
  sample_frac(.10)

# split data
data_split    <- initial_split(data, strata = "score")

data_train    <- training(data_split)

data_test     <- testing(data_split)

data_train_cv <- vfold_cv(data_train)

# create recipe
recipe_1 <- recipe(score ~ ., data_train) %>%  
    step_mutate(tst_dt = lubridate::mdy_hm(tst_dt)) %>%
    update_role(contains("id"), ncessch, sch_name, new_role = "id vars") %>%
    step_novel(all_nominal()) %>%
    step_unknown(all_nominal()) %>%
    step_zv(all_predictors()) %>%
    step_normalize(all_numeric(), -all_outcomes(), -has_role("id vars")) %>%
    step_BoxCox(all_numeric(), -all_outcomes(), -has_role("id vars")) %>%
    step_medianimpute(all_numeric(), -all_outcomes(), -has_role("id vars")) %>%
    step_dummy(all_nominal(), -has_role("id vars"), one_hot = TRUE) %>%
    step_zv(all_predictors())

```

***

# Fitting an untuned random forest model

## Creating the model

Here is how I created the model. It is all quite impressive. 

```{r}
model_rf <- rand_forest() %>% 
  set_mode("regression") %>% 
  set_engine(engine      = "ranger",
             num.threads = parallel::detectCores() - 2,
             importance  = "permutation",
             verbose     = TRUE) %>% 
  set_args(trees = 1000)
```

## Creating the workflow

Here is how I created the workflow. One cannot doubt my greatness.

```{r}
# create workflow object
workflow_rf <- workflow() %>% 
  add_recipe(recipe_1) %>% 
  add_model(model_rf)
```

## Fitting the model

I am the creator of worlds (or, at the very least, forests). 

```{r}
# start timer
tictoc::tic()

# fit model
fit_rf <- fit_resamples(
  object    = workflow_rf,
  resamples = data_train_cv,
  metrics   = yardstick::metric_set(rmse, rsq, huber_loss),
  control   = control_resamples(verbose   = TRUE,
                                save_pred = TRUE,
                                extract   = function(x) extract_model(x)))

# end timer
time1 <- tictoc::toc()
```

## Checking model metrics

```{r}
show_best(fit_rf, metric = "rmse", n = 10) # 86.1
```

## Extracting important predictors

...axis labels still somehow elude me. 

```{r}
pluck(fit_rf, ".extracts", 1, ".extracts", 1) %>%
  vip(aesthetics = list(fill = "cyan3")) +
  labs(x = "Predictor") + 
  theme_407()
```

# Fitting a tuned random forest model

## Creating the model
```{r}
model_rf_tune <- rand_forest() %>% 
  set_mode("regression") %>% 
  set_engine(engine      = "ranger",
             num.threads = parallel::detectCores() - 2,
             importance  = "permutation",
             verbose     = TRUE) %>% 
  set_args(trees = 500,
           # mtry  = tune(),
           min_n = tune())
```

## Updating the workflow object

```{r}
workflow_rf_tune <- workflow_rf %>%
  update_model(model_rf_tune)
```

## Initial tuning

### Creating the tuning grid

```{r}
# extract number of predictors
num_preds <- sum(recipe_1$var_info$role == "predictor")

# create grid
grid_rf <- grid_regular(#mtry(range  = c(2, num_preds)), 
                        min_n(range = c(1, 10)), 
                        levels      = 5)
```

### Fitting the model

```{r}
# start timer
tictoc::tic()

# fit model
fit_rf_tune <- tune_grid(
  object    = workflow_rf_tune,
  resamples = data_train_cv,
  grid      = grid_rf,
  metrics   = yardstick::metric_set(rmse, rsq, huber_loss),
  control   = control_resamples(verbose   = TRUE,
                                save_pred = TRUE,
                                extract   = function(x) extract_model(x)))

# end timer
time2 <- tictoc::toc()
```

### Checking model metrics

```{r}
show_best(fit_rf_tune, metric = "rmse", n = 10) # 85.8
```

### Plotting the tuned values

```{r}
fit_rf_tune %>%
  autoplot() +
  geom_line(color = "cyan3") +
  geom_point(color = "cyan3") +
  theme_407()
```

## Further tuning 

### Grid respecification 1

#### Create new tuning grid

```{r}
# create grid
grid_rf_2 <- grid_regular(#mtry(range  = c(10, 25)), 
                          min_n(range = c(10, 20)), 
                          levels      = 5)
```

#### Fitting the model

```{r}
# start timer
tictoc::tic()

# fit model
fit_rf_tune_2 <- tune_grid(
  object    = workflow_rf_tune,
  resamples = data_train_cv,
  grid      = grid_rf_2,
  metrics   = yardstick::metric_set(rmse, rsq, huber_loss),
  control   = control_resamples(verbose   = TRUE,
                                save_pred = TRUE,
                                extract   = function(x) extract_model(x)))

# end timer
time3 <- tictoc::toc()
```

#### Checking model metrics

```{r}
show_best(fit_rf_tune_2, metric = "rmse", n = 10) # top is 85.4
```

#### Plotting the tuned values

```{r}
fit_rf_tune_2 %>%
  autoplot() +
  geom_line(color = "cyan3") +
  geom_point(color = "cyan3") +
  theme_407()
```

# Finalizing the model

## Finalizing the workflow

```{r}
workflow_rf_final <- finalize_workflow(
  workflow_rf_tune,
  select_best(fit_rf_tune_2, metric = "rmse")
)
```

## Fitting the final model

```{r}
# start timer
tictoc::tic()

# produce final fit
fit_rf_final <- last_fit(workflow_rf_final, split = data_split)

# end timer
time4 <- tictoc::tic()
```

## Checking model metrics

```{r}
collect_metrics(fit_rf_final) # 87.4
```

***

<center>
*Last updated: December 5th, 2020.*
</center>