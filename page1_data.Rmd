---
title: "Tensor Flo Ridas: A Journey in Machine Learning"
output: 
  html_document:
    toc: TRUE
    toc_depth: 4
    toc_float: TRUE
    number_sections: FALSE
    code_folding: hide
    df_print: paged
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning = FALSE,
                      message = FALSE)

#install.packages("kableExtra")

# load required packages
library(tidyverse)
library(tidymodels)
library(here)
library(rio)
library(workflows)
library(tictoc)
library(kableExtra)

#turn off scientific notation
options(scipen = 999)
```

```{r load_theme}
theme_tensor_flo <- function() { 
    theme_bw() %+replace% 
        theme(
          legend.background = element_rect(fill = "#1b1d22",
                                           colour = "#1b1d22"),
          panel.background = element_rect(fill = "#171f24",
                                          colour = "#1b1d22",
                                          size = 0.5,
                                          linetype = "solid"),
          panel.grid.major = element_line(size = 0.2, 
                                          linetype = 'solid', 
                                          colour = "gray40"), 
          panel.grid.minor = element_line(size = 0.0, 
                                          linetype = 'solid',
                                          colour = "gray40"),
          axis.line = element_line(colour = "white"),
          plot.background = element_rect(fill = "#171f24"),
          text  = element_text(color = "white"),
          axis.text  = element_text(color = "white"),
          axis.ticks = element_line(color = "white"),
          strip.background = element_rect(fill = "white",
                                          colour = "white"))}
```

**Describe core features of the data, any additional data you joined in and why, and basic descriptives.**

**The description should be sufficiently clear that the instructor understands all the variables that were included in your modeling, and how the final analytic dataset was constructed, without actually viewing your code. While specific code snippets can be included in this section to add clarity, the reader should not have to rely on this code to understand the data preparation.**

**Provide a summary of the variables (e.g., how many categorical/continuous variables were included, what were the range of values, etc.), and any feature engineering applied to the data. If missing data are present, discuss how these values were handled, variable transformations, etc. Use this section to explain your data splitting process for model evaluation.**

# Our dependent variable

Across the entire United States, students in grades 3-8 are tested annually in reading and math. Our goal in this course was to create a machine learning model capable of accurately predicting a given student's performance on these tests (i.e., a scaled test total score). While our dataset is simulated from actual statewide testing administration across the state of Oregon, the overall distributions are highly similar. The school IDs (`ncessch`) are real. So we used school IDs to link our simulated data with other sources (e.g., [National Center for Educational Statistics](https://nces.ed.gov/ccd/files.asp#Fiscal:2,LevelId:7,SchoolYearId:33,Page:1)), which will be elaborated upon below.

# Simulated data predictors

As mentioned above, our dataset simulates real testing data in the state of Oregon. Below is a summary of the 39 predictor variables included in this dataset:

```{r import_dictionary}
dictionary <- import(here::here("data", "ASHdata_dictionary.xlsx"),
                     setclass = "tbl_df")
dictionary %>%
  kbl() %>%
  kable_styling(bootstrap_options = c("condensed", "responsive"),
                full_width = F) %>%
  row_spec(0, background = "#171F24", color = "white") %>% # Color header row background 
  column_spec(1, bold = T, border_right = T, background = "#171F24", color = "white") %>%
  column_spec(2, width = "40em", background = "#171F24", color = "white") %>%
  scroll_box(width = "100%", height = "225px")
```

$~$

And below you can find a summary of the descriptive statistics for each variable:

```{r import_data_main}
# read in data set
data <- read_csv("data/train.csv",
                 col_types = cols(.default = col_guess(), 
                                  calc_admn_cd = col_character())) %>% 
    select(-classification)

psych::describe(data)
```

$~$

# Additional data sources

## Free and reduced lunch

Note that our simulated dataset described above primarily includes student-level predictors. We decided to import additional data containing info specific to the schools (`ncessch`) students attend. We reasoned that school-level variables could have an important impact on the quality of education students receive, in turn impacting students' scores. 

First, we obtained free/reduced lunch data for each school and counted the number of students at each school. In doing so, our goal was to create two new predictor variables that we could add to our existing dataset. Namely, we computed variables that reflect the proportion of students at a given school who are eligible for free (`free_lunch_prop`) or reduced (`reduced_lunch_prop`) lunches.

```{r import_frl_data}
# read in free/reduced lunch data and student counts
frl <- import(here("data", "lunch.csv"),
              setclass = "tbl_df")  %>% 
  janitor::clean_names()  %>% 
  filter(st == "OR")  %>%
  select(ncessch, lunch_program, student_count)  %>% 
  mutate(student_count = replace_na(student_count, 0))  %>% 
  pivot_wider(names_from  = lunch_program,
              values_from = student_count)  %>% 
  janitor::clean_names()  %>% 
  mutate(ncessch = as.double(ncessch))

stu_counts <- import("https://github.com/datalorax/ach-gap-variability/raw/master/data/achievement-gaps-geocoded.csv",
                     setclass = "tbl_df")  %>% 
  filter(state == "OR" & year == 1718)  %>% 
  count(ncessch, wt = n)  %>% 
  mutate(ncessch = as.double(ncessch))

frl <- left_join(frl, stu_counts)

rm(stu_counts)

frl <- frl %>% 
  mutate(free_lunch_prop = free_lunch_qualified / n,
         reduced_lunch_prop = reduced_price_lunch_qualified / n)  %>% 
  select(ncessch, ends_with("prop"))

psych::describe(frl)
```

## Staff

Here we obtained data on the number of full-time-equivalent (FTE) teachers at each school (`teachers`). Schools with more funding seemingly offer more resources and newer, better equipment for students--possibly contributing to better test scores. Our rationale was that the number of full-time teachers at a school could potentially serve as an indirect index of school funding. Or, perhaps the number of full time teachers could be negatively associated with class room size. If smaller class rooms are associated with better student performance, then perhaps this is another means by which the number of full-time teachers could aid in the predictive utility of our model.

```{r import_staff_data}
# read in staff data
staff <- import(here("data", "staff.csv"), setclass = "tbl_df")  %>% 
  janitor::clean_names()  %>% 
  filter(st == "OR")  %>%
  mutate(ncessch = as.double(ncessch)) %>%
  select(ncessch, teachers)

psych::describe(staff)
```

## Other school characteristics

Here, we sought to add three more predictors. The first, `titlei_status` refers to a given school's Title I status. A Title 1 school receives federal funds for students. The premise of Title 1 is that schools with large numbers of low-income students will receive supplemental funds to help ensure that all children meet challenging state academic standards. Note schools may receive a classification other than being a Title I school or not being a Title I school. Schools may also be classified as Title I targeted assistance eligible schools, Title I school-wide eligible Title I targeted assistance schools, and more. 

The second predictor of interest was `nslp_status`, which reflects a given school's National School Lunch Program (NSLP) status. The [NSLP](https://www.fns.usda.gov/nslp) "is a federally assisted meal program operating in public and nonprofit private schools and residential child care institutions". Children from families with incomes at or below 130 percent of the Federal Poverty Level (FPL) are eligible for free school meals, whereas children from families with incomes between 130 to 185 percent FPL qualify for reduced-price meals. Approximately [95 percent](https://frac.org/wp-content/uploads/cnnslp.pdf) of public schools participate in NSLP in some way. Schools may fall under codes reflecting participation in NSLP without using any Provision or Community Eligibility Option, participation in NSLP under Community Eligibility Option, participation in NSLP under Provision 1, etc.

The final predictor, `virtual`, refers to a school's virtual (online) status. Some schools may be fully virtual, not virtual at all, supplemental virtual, or virtual with face to face options.

```{r import_school_data}
# read in school characteristics data
school_chars <- import(here("data", "school_characteristics.csv"), 
                       setclass = "tbl_df")  %>% 
  janitor::clean_names()  %>% 
  filter(st == "OR")  %>%
  mutate(ncessch = as.double(ncessch)) %>%
  select(ncessch, titlei_status, nslp_status, virtual)

psych::describe(school_chars)
```

## Ethinicity data

Finally, we imported data that breaks down the ethnic makeup of the students enrolled at each school. Specifically, for each school, we know the proportion of students who identify as:

+ American Indian/Alaska Native (`p_american_indian_alaska_native`)
+ Asian (`p_asian`)
+ African American (`p_black_african_american`)
+ Hispanic/Latino (`p_hispanic_latino`)
+ Native Hawaiian/Pacific Islander (`p_native_hawaiian_pacific_islander`)
+ White (`p_white`)
+ Multiracial (`p_multiracial`)

```{r import_ethnicity_data}
# read in ethnicity data
eth <- import(file = here::here("data",
                                "fallmembershipreport_20192020.xlsx"),
              sheet = "School (19-20)",
              setclass = "tibble") %>%
  janitor::clean_names() %>%
  select(attnd_schl_inst_id = attending_school_id,
         sch_name = school_name,
         matches("percent"))

names(eth) <- gsub("x2019_20_percent", "p", names(eth)) 

psych::describe(eth)
```

```{r merge_data}
# combine our data with frl, staff, school characteristics, and ethnicities
data <- data %>%
    left_join(frl) %>%
    left_join(staff) %>%
    left_join(school_chars) %>%
    left_join(eth)

# remove unneeded dataframes
rm(frl, staff, school_chars, eth)
```

# Getting to know our data

## Create splits

The goal of machine learning is to predict results based on new (unseen) data. The simplest way to do this is to split our data into two parts via the function `initial split()`: an initial training set and a test set. We will eventually fit a model to the training data and predict the results of the test set. After the initial split, we need to resample our training set to create subsets of "new" data samples that we can train the model on. The most common method--which we use here--is 10-fold cross-validation. Using `vfold_cv()`, we randomly split the training data into 10 distinct samples ("folds") of approximately equal size. Importantly, each fold contains a sample in which none of the observations are repeated in other folds. And within each fold, a random 10% (1/10) of training data are sampled for an assessment (test) set, whereas the  remaining 90% of the training data serve as the analysis (training) set.

```{r cv_split}
set.seed(3000)

# data %<>%
#   select(-matches("lat|lon|native|multi|black"))

# data <- data %>%
#   slice_sample(prop = .01)

data_split <- initial_split(data)

data_train <- training(data_split)

data_test  <- testing(data_split)

data_train_cv <- vfold_cv(data_train)
```

### Exploring the data

```{r histograms, fig.height=7, fig.width=10}
hist_data <- data_train %>%
  select(where(is.numeric),
         -contains("id"),
         -ncessch) %>%
  dplyr::rename("Grade" = `enrl_grd`,
                "Latitude" = `lat`,
                "Longitude" = `lon`,
                "Score" = `score`,
                "Number of Teachers" = `teachers`,
                "Proportion Free Lunch" = `free_lunch_prop`,
                "Proportion Reduced Lunch" = `reduced_lunch_prop`,
                "Proportion Indigenous" = `p_american_indian_alaska_native`,
                "Proportion Asian" = `p_asian`,
                "Proportion African American" = `p_black_african_american`,
                "Proportion Hispanic" = `p_hispanic_latino`,
                "Proportion Pacific Islander" = `p_native_hawaiian_pacific_islander`,
                "Proportion Multiracial" = `p_multiracial`,
                "Proportion White" = `p_white`) %>%
  gather("variable")

ggplot(hist_data, aes(x = value)) +
    geom_histogram(color = "#171f24", 
                   fill = "cyan3",
                   alpha = 0.70) +
    facet_wrap(~variable,
               ncol = 4,
               scales = "free") +
    theme_bw() +
    theme_tensor_flo() +
    labs(y = "Count",
         x = "Value")
```

```{r zo, fig.height=7, fig.width=10}
cor_data <- data_train %>%
  select(where(is.numeric),
         -contains("id"),
         -ncessch) %>%
  dplyr::rename("Grade" = `enrl_grd`,
                "Latitude" = `lat`,
                "Longitude" = `lon`,
                "Score" = `score`,
                "Number of Teachers" = `teachers`,
                "Prop Free Lunch" = `free_lunch_prop`,
                "Prop Reduced Lunch" = `reduced_lunch_prop`,
                "Prop Indigenous" = `p_american_indian_alaska_native`,
                "Prop Asian" = `p_asian`,
                "Prop African American" = `p_black_african_american`,
                "Prop Hispanic" = `p_hispanic_latino`,
                "Prop Pacific Islander" = `p_native_hawaiian_pacific_islander`,
                "Prop Multiracial" = `p_multiracial`,
                "Prop White" = `p_white`) %>%
  cor(use = "pairwise.complete.obs") %>%
  data.frame() %>%
  rownames_to_column() %>%
  gather("colname", "cor", -rowname)
  
  
ggplot(cor_data, aes(x = rowname, y = colname, fill = cor)) +
    geom_tile(color = "white") +
    geom_text(aes(label=round(cor,2))) + 
    scale_fill_gradient2(low = "#FCCF47", 
                         mid = "white", 
                         high = "#01C0C0") +
    theme_tensor_flo() +
    theme(axis.text.x = element_text(angle = 45, hjust = 1),
          axis.text = element_text(size = 10.5)) +
    labs(y = "Variable",
         x = "Variable")
```

### Build the preliminary recipe

```{r rec_1}
recipe_1 <- recipe(score ~ ., data_train) %>%  
    step_mutate(tst_dt = lubridate::mdy_hm(tst_dt)) %>%
    update_role(contains("id"), ncessch, sch_name, new_role = "id vars") %>%
    step_novel(all_nominal()) %>%
    step_unknown(all_nominal()) %>%
    step_zv(all_predictors()) %>%
    step_normalize(all_numeric(), -all_outcomes(), -has_role("id vars")) %>%
    step_BoxCox(all_numeric(), -all_outcomes(), -has_role("id vars")) %>%
    step_medianimpute(all_numeric(), -all_outcomes(), -has_role("id vars")) %>%
    step_dummy(all_nominal(), -has_role("id vars"), one_hot = TRUE) %>%
    step_zv(all_predictors())
```

***

<center>
*Last updated: December 6th, 2020.*
</center>