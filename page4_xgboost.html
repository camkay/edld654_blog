<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Tensor Flo Ridas: A Journey in ML</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/navigation-1.1/codefolding.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/pagedtable-1.1/css/pagedtable.css" rel="stylesheet" />
<script src="site_libs/pagedtable-1.1/js/pagedtable.js"></script>
<link href="site_libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="site_libs/anchor-sections-1.0/anchor-sections.js"></script>
<link href="site_libs/font-awesome-5.1.0/css/all.css" rel="stylesheet" />
<link href="site_libs/font-awesome-5.1.0/css/v4-shims.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="custom.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">EDLD654</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">
    <span class="fa fa-home"></span>
     
    Home
  </a>
</li>
<li>
  <a href="page1_data.html">
    <span class="fa fa-database"></span>
     
    The Data
  </a>
</li>
<li>
  <a href="page2_linear.html">
    <span class="fa fa-chart-line"></span>
     
    Linear Model
  </a>
</li>
<li>
  <a href="page3_rf.html">
    <span class="fa fa-tree"></span>
     
    Random Forest
  </a>
</li>
<li>
  <a href="page4_xgboost.html">
    <span class="fa fa-rocket"></span>
     
    Boosted Tree
  </a>
</li>
<li>
  <a href="page5_comparison.html">
    <span class="fa fa-balance-scale"></span>
     
    Model Comparisons
  </a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/camkay/edld654_blog">
    <span class="fa fa-github"></span>
     
    GitHub
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">

<div class="btn-group pull-right">
<button type="button" class="btn btn-default btn-xs dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>



<h1 class="title toc-ignore">Tensor Flo Ridas: A Journey in ML</h1>

</div>


<hr />
<div id="intro-to-boosted-trees" class="section level1">
<h1>Intro to Boosted Trees</h1>
<div id="what-are-boosted-trees" class="section level2">
<h2>What are boosted trees?</h2>
<p>The third model we chose to build was a boosted tree model. Similar to random forest, the boosted tree model builds decision trees over a number of iterations. While the random forest utilizes bootstrap resampling (i.e. bagging) and aggregates the predictions across samples, boosted trees learn sequentially. On each iteration, a tree is built to predict the residuals from the tree before, resulting in a slow learning process.</p>
<p>Boosted trees can lead to overfitting of the data if not tuned properly and can be computationally inefficient. A benefit to the boosted tree model, however, is that the model can be stopped when learning does not surpass a specified threshold, preventing it from overfitting. In fact, boosted trees are known for having some of the best out-of-box performance when predicting tabular data. For this reason, we will give it a try.</p>
</div>
</div>
<div id="data" class="section level1">
<h1>Data</h1>
<div id="import-data" class="section level2">
<h2>Import data</h2>
<p>Only 1% of the training data was used so the model tuning wasn’t too computationally demanding. The subset data was split into a training set and a testing set. The training set was used to tune the cross-validated models, while the testing set was used to evaluate it’s performance.</p>
<pre class="r"><code># Import joined data file
data &lt;- import(here(&quot;data&quot;,&quot;data.csv&quot;),
               setclass = &quot;tbl_df&quot;)

# Subset data
data_sub &lt;- data %&gt;% 
  sample_frac(.1)

# Split data
splits &lt;- initial_split(data_sub, strata = &quot;score&quot;)
train &lt;- training(splits)
test &lt;- testing(splits)</code></pre>
</div>
<div id="specify-recipe" class="section level2">
<h2>Specify recipe</h2>
<p>Note the recipe is based off the entire set of training data, but only applied to the subset. This is so imputations are not specific to just a sample of the training data.</p>
<pre class="r"><code>rec &lt;- recipe(score ~ ., train) %&gt;%  
    step_mutate(tst_dt = lubridate::mdy_hm(tst_dt)) %&gt;%
    update_role(contains(&quot;id&quot;), ncessch, sch_name, new_role = &quot;id vars&quot;) %&gt;%
    step_novel(all_nominal()) %&gt;%
    step_unknown(all_nominal()) %&gt;%
    step_zv(all_predictors()) %&gt;%
    step_normalize(all_numeric(), -all_outcomes(), -has_role(&quot;id vars&quot;)) %&gt;%
    step_BoxCox(all_numeric(), -all_outcomes(), -has_role(&quot;id vars&quot;)) %&gt;%
    step_medianimpute(all_numeric(), -all_outcomes(), -has_role(&quot;id vars&quot;)) %&gt;%
    step_dummy(all_nominal(), -has_role(&quot;id vars&quot;), one_hot = TRUE) %&gt;%
    step_zv(all_predictors())</code></pre>
</div>
<div id="preprocess-data" class="section level2">
<h2>Preprocess data</h2>
<p>For the boosted tree model, the <code>xgboost</code> package was used. Since <code>tidymodels</code> was not used, the data had to be preprocessed before fitting the model. Additional steps were also required, including transforming the preprocessed data into a “feature matrix.” This matrix is the input for the model, which only includes predictors. The scores associated with each row were saved separately into a vector called <code>outcome</code>.</p>
<p>Note: Converting <code>data_sub_baked</code> to a matrix converted everything to a character. This was likely because of the <code>tst_dt</code> variable. To transform the data into an acceptable format for the feature matrix, the <code>tst_dt</code> variable was first converted to a numeric type.</p>
<pre class="r"><code># Preprocess data
train_baked &lt;- rec %&gt;% 
  prep() %&gt;% 
  bake(train)

# Transform tst_dt to numeric
train_baked$tst_dt &lt;- as.numeric(train_baked$tst_dt)

# Transform preprocessed data into feature matrix
features &lt;- train_baked %&gt;% 
  select(-score, -contains(&quot;id&quot;), -ncessch, -sch_name) %&gt;% 
  as.matrix()

# Saving the scores as a separate vector
outcome &lt;- train_baked$score</code></pre>
</div>
</div>
<div id="model-tuning" class="section level1">
<h1>Model Tuning</h1>
<p>Boosted trees have considerably more hyperparameters than random forest. For the purpose of our project, we chose what we felt were the most important hyperparameters:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Number of trees</strong> - If too many trees are used, then our model can overfit the data. This would result in poor out-of-box performance. Alternatively, too few trees may not allow our model enough time to learn.</p></li>
<li><p><strong>Learning rate</strong> - Boosted trees often utilize a learning process called <strong>gradient descent</strong>. On each iteration, the model is adjusted in a way that most reduces the cost function (e.g. RMSE). The amount in which the model is adjusted on each iteration is determined by the learning rate. When the learning rate is high, the model can make adjustments that are too large and end up being far from the optimal solution. When the learning rate is too low, the model may not reach the optimal solution within the specified number of trees.</p></li>
<li><p><strong>Tree depth</strong> - This refers to the number of splits for each tree. In contrast to random forest, shallow trees are used, typically with 1 to 6 splits. A shallower tree would require more trees to reach the optimal solution, while a deeper tree can lead to overfitting.</p></li>
<li><p><strong>Randomness</strong> - There are a number of ways in which randomness can be introduced to a boosted tree model. One is sampling the number of predictors used to determine each split within a tree. This is similar to the random forest model, in which the number of predictors sampled for each tree (<code>mtry</code>) is a hyperparameter.</p></li>
</ol>
<div id="basic-model-without-any-tuning" class="section level2">
<h2>Basic model without any tuning</h2>
<p>Before tuning the boosted tree model, a single-cross validated model with default hyperparameters was run to estimate the timing.</p>
<p>The default boosted * 100 trees * early stop after 20 iterations of no learning * 10-fold cross validation</p>
<pre class="r"><code>tic() 

fit_def_xgb &lt;- xgb.cv(
  data = features,
  label = outcome,
  nrounds = 100,
  objective = &quot;reg:squarederror&quot;,
  early_stopping_rounds = 20,
  nfold = 10,
  verbose = 0
)

time_default &lt;- toc()</code></pre>
<pre><code>## 32.344 sec elapsed</code></pre>
<p>Note the cross-validated boosted tree model with default parameters took about 42.467 s. Will proceed with caution.</p>
<p>Below are the mean train and test metrics from each iteration (i.e. tree). The model is evaluated using root mean squared error (RMSE).</p>
<pre class="r"><code>fit_def_xgb$evaluation_log</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["iter"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["train_rmse_mean"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["train_rmse_std"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["test_rmse_mean"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["test_rmse_std"],"name":[5],"type":["dbl"],"align":["right"]}],"data":[{"1":"1","2":"1752.99248","3":"0.31122361","4":"1752.98929","5":"3.645028"},{"1":"2","2":"1229.16432","3":"0.21859303","4":"1229.16807","5":"3.526148"},{"1":"3","2":"863.04202","3":"0.15320200","4":"863.02264","5":"3.293723"},{"1":"4","2":"607.63948","3":"0.11186592","4":"607.60599","5":"3.067641"},{"1":"5","2":"430.08083","3":"0.08022433","4":"430.06128","5":"2.854647"},{"1":"6","2":"307.42892","3":"0.07607753","4":"307.53954","5":"2.671737"},{"1":"7","2":"223.70516","3":"0.10337513","4":"224.14133","5":"2.452093"},{"1":"8","2":"167.78387","3":"0.14340424","4":"168.70430","5":"2.183784"},{"1":"9","2":"131.65229","3":"0.19670406","4":"133.21523","5":"1.913329"},{"1":"10","2":"109.42771","3":"0.24368872","4":"111.62468","5":"1.686907"},{"1":"11","2":"96.34620","3":"0.24999347","4":"99.34134","5":"1.556759"},{"1":"12","2":"88.97797","3":"0.31263260","4":"92.63290","5":"1.566794"},{"1":"13","2":"84.84867","3":"0.27240874","4":"89.08329","5":"1.569422"},{"1":"14","2":"82.55650","3":"0.29760290","4":"87.26637","5":"1.571138"},{"1":"15","2":"81.18417","3":"0.30952482","4":"86.34523","5":"1.634958"},{"1":"16","2":"80.26725","3":"0.33236033","4":"85.89652","5":"1.666730"},{"1":"17","2":"79.70705","3":"0.29727659","4":"85.64563","5":"1.711599"},{"1":"18","2":"79.17228","3":"0.33959925","4":"85.50640","5":"1.763893"},{"1":"19","2":"78.76169","3":"0.36363178","4":"85.42071","5":"1.779175"},{"1":"20","2":"78.48082","3":"0.36183463","4":"85.40479","5":"1.787812"},{"1":"21","2":"78.17980","3":"0.36774585","4":"85.39350","5":"1.786599"},{"1":"22","2":"77.87883","3":"0.35632578","4":"85.36794","5":"1.786957"},{"1":"23","2":"77.57000","3":"0.31483033","4":"85.34033","5":"1.771347"},{"1":"24","2":"77.32270","3":"0.30512229","4":"85.36130","5":"1.820825"},{"1":"25","2":"77.05139","3":"0.28824645","4":"85.35479","5":"1.822410"},{"1":"26","2":"76.84768","3":"0.24569473","4":"85.36774","5":"1.831363"},{"1":"27","2":"76.56071","3":"0.28877289","4":"85.40163","5":"1.853509"},{"1":"28","2":"76.37078","3":"0.25694697","4":"85.42083","5":"1.836795"},{"1":"29","2":"76.12395","3":"0.29004609","4":"85.47931","5":"1.849756"},{"1":"30","2":"75.92206","3":"0.30552371","4":"85.50039","5":"1.868886"},{"1":"31","2":"75.63741","3":"0.29909677","4":"85.50998","5":"1.887949"},{"1":"32","2":"75.44246","3":"0.29378627","4":"85.52794","5":"1.883337"},{"1":"33","2":"75.17122","3":"0.30425254","4":"85.55324","5":"1.887953"},{"1":"34","2":"74.93875","3":"0.31179889","4":"85.57852","5":"1.925295"},{"1":"35","2":"74.74753","3":"0.33075864","4":"85.61659","5":"1.937610"},{"1":"36","2":"74.55586","3":"0.34292897","4":"85.67357","5":"1.944379"},{"1":"37","2":"74.35279","3":"0.35162016","4":"85.70637","5":"1.991464"},{"1":"38","2":"74.17820","3":"0.35080779","4":"85.72162","5":"1.998761"},{"1":"39","2":"73.96066","3":"0.37948689","4":"85.73917","5":"2.007640"},{"1":"40","2":"73.73656","3":"0.38844920","4":"85.76857","5":"1.995452"},{"1":"41","2":"73.53361","3":"0.38363985","4":"85.79642","5":"1.995831"},{"1":"42","2":"73.35296","3":"0.43580644","4":"85.81596","5":"2.015297"},{"1":"43","2":"73.12664","3":"0.46123851","4":"85.85521","5":"2.012604"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<pre class="r"><code>fit_def_log &lt;- fit_def_xgb$evaluation_log %&gt;% 
  pivot_longer(-iter,
               names_to = c(&quot;set&quot;,&quot;metric&quot;,&quot;measure&quot;),
               names_sep = &quot;_&quot;) %&gt;% 
  pivot_wider(names_from = &quot;measure&quot;,
              values_from = &quot;value&quot;) %&gt;% 
  filter(iter &gt; 5)

ggplot(fit_def_log, aes(iter, mean)) +
  geom_line(aes(color = set), size = 1) +
  labs(x = &quot;Tree Iteration&quot;, y = &quot;Mean RMSE&quot;) +
  scale_color_manual(values = c(&quot;#FFDB6D&quot;, &quot;#00AFBB&quot;)) +
  theme_407()</code></pre>
<p><img src="page4_xgboost_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<pre class="r"><code>fit_def_xgb$best_iteration</code></pre>
<pre><code>## [1] 23</code></pre>
<pre class="r"><code>fit_def_xgb$evaluation_log %&gt;% 
  dplyr::slice(fit_def_xgb$best_iteration)</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["iter"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["train_rmse_mean"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["train_rmse_std"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["test_rmse_mean"],"name":[4],"type":["dbl"],"align":["right"]},{"label":["test_rmse_std"],"name":[5],"type":["dbl"],"align":["right"]}],"data":[{"1":"23","2":"77.57","3":"0.3148303","4":"85.34033","5":"1.771347"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<pre class="r"><code>mt_rmse &lt;- fit_def_xgb$evaluation_log %&gt;% 
  dplyr::slice(fit_def_xgb$best_iteration) %&gt;% 
  select(test_rmse_mean)</code></pre>
<p>Even with the default parameters, the mean RMSE across the test sets is relatively good (85.34). Let’s see how well the model will do with some tuning!</p>
</div>
<div id="tune-number-of-trees" class="section level2">
<h2>Tune number of trees</h2>
<ul>
<li>start at high learning rate (shrinkage = .1)</li>
</ul>
<pre class="r"><code>grid_tree = expand.grid(num_tree = seq(100, 1000, length.out = 5))

tic()
fit_tune_trees &lt;- map(grid_tree$num_tree, ~ {
 xgb.cv(
   data = features,
   label = outcome,
   nrounds = .x, # number of trees
   objective = &quot;reg:squarederror&quot;, 
   early_stopping_rounds = 20, 
   nfold = 10,
   params = list(eta = .1),
   verbose = 0
 ) 
})
toc()</code></pre>
<pre><code>## 350.766 sec elapsed</code></pre>
<p>Below are the results of the tuned models:</p>
<pre class="r"><code>fit_tune_tree_log &lt;- map_df(fit_tune_trees, ~{
  .x$evaluation_log %&gt;% 
  pivot_longer(-iter,
               names_to = c(&quot;set&quot;,&quot;metric&quot;,&quot;measure&quot;),
               names_sep = &quot;_&quot;) %&gt;% 
  pivot_wider(names_from = &quot;measure&quot;,
              values_from = &quot;value&quot;)
}, .id = &quot;num_tree&quot;) %&gt;% 
  mutate(num_tree = factor(num_tree, 
                           labels = str_c(as.character(grid_tree$num_tree), &quot;trees&quot;, sep = &quot; &quot;))) %&gt;% 
  filter(iter &gt; 30)

ggplot(fit_tune_tree_log, aes(iter, mean)) +
  geom_line(aes(color = set), size = 1) +
  labs(x = &quot;Tree Iteration&quot;, y = &quot;Mean RMSE&quot;) +
  scale_color_manual(values = c(&quot;#FFDB6D&quot;, &quot;#00AFBB&quot;)) +
  theme_407() +
  facet_grid(~num_tree) +
  theme(axis.text.x = element_text(size = 8))</code></pre>
<p><img src="page4_xgboost_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<pre class="r"><code>(tune_tree_best &lt;- map_df(fit_tune_trees, ~{
  .x$evaluation_log %&gt;% 
  dplyr::slice(.x$best_iteration)
}) %&gt;% 
  mutate(num_trees = grid_tree$num_tree) %&gt;% 
  arrange(test_rmse_mean) %&gt;% 
  select(num_trees, iter, test_rmse_mean, test_rmse_std))</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["num_trees"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["iter"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["test_rmse_mean"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["test_rmse_std"],"name":[4],"type":["dbl"],"align":["right"]}],"data":[{"1":"1000","2":"88","3":"84.85771","4":"1.987255"},{"1":"550","2":"79","3":"84.89951","4":"1.980424"},{"1":"775","2":"86","3":"84.94128","4":"1.030072"},{"1":"325","2":"78","3":"84.95627","4":"2.039366"},{"1":"100","2":"84","3":"84.95898","4":"1.703780"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<pre class="r"><code>best_numtrees &lt;- tune_tree_best$num_trees[1]</code></pre>
</div>
<div id="tune-learning-rate-pt.-1" class="section level2">
<h2>Tune learning rate (pt. 1)</h2>
<p>Typical learning rate is .001 - .3</p>
<ul>
<li>use optimal number of trees</li>
</ul>
<pre class="r"><code>grid_learn &lt;- expand.grid(learn_rate = seq(.001, .3, length.out = 10))

tic()
fit_tune_learn1 &lt;- map(grid_learn$learn_rate, ~ {
 xgb.cv(
   data = features,
   label = outcome,
   nrounds = 100, # number of trees
   objective = &quot;reg:squarederror&quot;,
   early_stopping_rounds = 20, 
   nfold = 10,
   params = list(eta = .x),
   verbose = 0
 ) 
})
toc()</code></pre>
<pre><code>## 516.128 sec elapsed</code></pre>
<p>Below are the results of the tuned models:</p>
<pre class="r"><code>fit_tune_learn1_log &lt;- map_df(fit_tune_learn1, ~{
  .x$evaluation_log %&gt;% 
  pivot_longer(-iter,
               names_to = c(&quot;set&quot;,&quot;metric&quot;,&quot;measure&quot;),
               names_sep = &quot;_&quot;) %&gt;% 
  pivot_wider(names_from = &quot;measure&quot;,
              values_from = &quot;value&quot;)
}, .id = &quot;learn_rate&quot;) %&gt;% 
  mutate(learn_rate = factor(learn_rate, 
                             levels = as.character(seq(1,10)),
                             labels = as.character(round(grid_learn$learn_rate,3)))) %&gt;% 
  filter(iter &gt; 10)

ggplot(fit_tune_learn1_log, aes(iter, mean)) +
  geom_line(aes(color = set), size = 1) +
  labs(x = &quot;Tree Iteration&quot;, y = &quot;Mean RMSE&quot;) +
  scale_color_manual(values = c(&quot;#FFDB6D&quot;, &quot;#00AFBB&quot;)) +
  theme_407() +
  facet_wrap(~learn_rate, nrow = 2) +
  theme(axis.text.x = element_text(size = 6))</code></pre>
<p><img src="page4_xgboost_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<pre class="r"><code>(tune_learn1_best &lt;- map_df(fit_tune_learn1, ~{
  .x$evaluation_log %&gt;% 
  dplyr::slice(.x$best_iteration)
}) %&gt;% 
  mutate(learning_rate = grid_learn$learn_rate) %&gt;% 
  arrange(test_rmse_mean) %&gt;% 
  select(learning_rate, iter, test_rmse_mean, test_rmse_std))</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["learning_rate"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["iter"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["test_rmse_mean"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["test_rmse_std"],"name":[4],"type":["dbl"],"align":["right"]}],"data":[{"1":"0.13388889","2":"68","3":"84.83468","4":"1.3842441"},{"1":"0.06744444","2":"100","3":"84.96475","4":"2.0043223"},{"1":"0.10066667","2":"80","3":"84.98153","4":"1.4298061"},{"1":"0.16711111","2":"52","3":"85.04621","4":"1.9157030"},{"1":"0.20033333","2":"37","3":"85.11147","4":"1.4662390"},{"1":"0.23355556","2":"31","3":"85.25853","4":"0.9789315"},{"1":"0.26677778","2":"24","3":"85.39309","4":"2.6648228"},{"1":"0.30000000","2":"26","3":"85.51075","4":"2.5124518"},{"1":"0.03422222","2":"100","3":"115.44553","4":"2.0982556"},{"1":"0.00100000","2":"100","3":"2264.23364","4":"3.3636672"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<pre class="r"><code>best_learnrate1 &lt;- tune_learn1_best$learning_rate[1]</code></pre>
</div>
<div id="tune-tree-depth" class="section level2">
<h2>Tune tree depth</h2>
<ul>
<li>use optimal number of trees</li>
<li>use optimal learning rate</li>
</ul>
<p>Defined as <code>max_depth</code> in <code>XGBoost</code></p>
<pre class="r"><code>grid_depth &lt;- expand.grid(tree_depth = seq(1,6))

tic()
fit_tune_depth &lt;- map(grid_depth$tree_depth, ~ {
 xgb.cv(
   data = features,
   label = outcome,
   nrounds = best_numtrees, # number of trees
   objective = &quot;reg:squarederror&quot;, 
   early_stopping_rounds = 20, 
   nfold = 10,
   params = list(eta = best_learnrate1,
                 max_depth = .x),
   verbose = 0
 ) 
})
toc()</code></pre>
<pre><code>## 629.672 sec elapsed</code></pre>
<p>Below are the results of the tuned models:</p>
<pre class="r"><code>fit_tune_depth_log &lt;- map_df(fit_tune_depth, ~{
  .x$evaluation_log %&gt;% 
  pivot_longer(-iter,
               names_to = c(&quot;set&quot;,&quot;metric&quot;,&quot;measure&quot;),
               names_sep = &quot;_&quot;) %&gt;% 
  pivot_wider(names_from = &quot;measure&quot;,
              values_from = &quot;value&quot;)
}, .id = &quot;tree_depth&quot;) %&gt;% 
  filter(iter &gt; 20)

ggplot(fit_tune_depth_log, aes(iter, mean)) +
  geom_line(aes(color = set), size = 1) +
  labs(x = &quot;Tree Iteration&quot;, y = &quot;Mean RMSE&quot;) +
  scale_color_manual(values = c(&quot;#FFDB6D&quot;, &quot;#00AFBB&quot;)) +
  theme_407() +
  facet_grid(~tree_depth) +
  theme(axis.text.x = element_text(size = 6))</code></pre>
<p><img src="page4_xgboost_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<pre class="r"><code>(tune_depth_best &lt;- map_df(fit_tune_depth, ~{
  .x$evaluation_log %&gt;% 
  dplyr::slice(.x$best_iteration)
}) %&gt;% 
  mutate(tree_depth = grid_depth$tree_depth) %&gt;% 
  arrange(test_rmse_mean) %&gt;% 
  select(tree_depth, iter, test_rmse_mean, test_rmse_std))</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["tree_depth"],"name":[1],"type":["int"],"align":["right"]},{"label":["iter"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["test_rmse_mean"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["test_rmse_std"],"name":[4],"type":["dbl"],"align":["right"]}],"data":[{"1":"3","2":"191","3":"84.55619","4":"0.9771931"},{"1":"4","2":"127","3":"84.62394","4":"2.4746137"},{"1":"2","2":"341","3":"84.81347","4":"1.7877110"},{"1":"5","2":"86","3":"84.86217","4":"1.5489517"},{"1":"6","2":"61","3":"84.89459","4":"2.6723767"},{"1":"1","2":"962","3":"85.55398","4":"1.4386002"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<pre class="r"><code>best_depth&lt;- tune_depth_best$tree_depth[1]</code></pre>
</div>
<div id="tune-randomness" class="section level2">
<h2>Tune randomness</h2>
<ul>
<li>use optimal number of trees</li>
<li>use optimal learning rate</li>
<li>use optimal tree depth</li>
</ul>
<p>Defined as <code>colsample_bytree</code> in <code>XGBoost</code> This parameter determines the number of features to sample for each new tree, similar to <code>mtry</code> in <code>tidymodels</code>.</p>
<pre class="r"><code>grid_randcol &lt;- expand.grid(colsample = seq(.1, 1, length.out = 5))

tic()
fit_tune_randcol &lt;- map(grid_randcol$colsample, ~ {
 xgb.cv(
   data = features,
   label = outcome,
   nrounds = best_numtrees, # number of trees
   objective = &quot;reg:squarederror&quot;, 
   early_stopping_rounds = 20, 
   nfold = 10,
   params = list(eta = best_learnrate1,
                 max_depth = best_depth,
                 colsample_bytree = .x),
   verbose = 0
 ) 
})
toc()</code></pre>
<pre><code>## 358.136 sec elapsed</code></pre>
<p>Below are the results of the tuned models:</p>
<pre class="r"><code>fit_tune_randcol_log &lt;- map_df(fit_tune_randcol, ~{
  .x$evaluation_log %&gt;% 
  pivot_longer(-iter,
               names_to = c(&quot;set&quot;,&quot;metric&quot;,&quot;measure&quot;),
               names_sep = &quot;_&quot;) %&gt;% 
  pivot_wider(names_from = &quot;measure&quot;,
              values_from = &quot;value&quot;)
}, .id = &quot;col_sample&quot;) %&gt;% 
  mutate(col_sample = factor(col_sample, 
                             labels = as.character(grid_randcol$colsample))) %&gt;% 
  filter(iter &gt; 25)

ggplot(fit_tune_randcol_log, aes(iter, mean)) +
  geom_line(aes(color = set), size = 1) +
  labs(x = &quot;Tree Iteration&quot;, y = &quot;Mean RMSE&quot;) +
  scale_color_manual(values = c(&quot;#FFDB6D&quot;, &quot;#00AFBB&quot;)) +
  theme_407() +
  facet_grid(~col_sample) +
  theme(axis.text.x = element_text(size = 6))</code></pre>
<p><img src="page4_xgboost_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<pre class="r"><code>(tune_randcol_best &lt;- map_df(fit_tune_randcol, ~{
  .x$evaluation_log %&gt;% 
  dplyr::slice(.x$best_iteration)
}) %&gt;% 
  mutate(colsample = grid_randcol$colsample) %&gt;% 
  arrange(test_rmse_mean) %&gt;% 
  select(colsample, iter, test_rmse_mean, test_rmse_std))</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["colsample"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["iter"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["test_rmse_mean"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["test_rmse_std"],"name":[4],"type":["dbl"],"align":["right"]}],"data":[{"1":"0.100","2":"375","3":"84.48284","4":"2.073989"},{"1":"0.325","2":"209","3":"84.55040","4":"2.023183"},{"1":"0.550","2":"193","3":"84.61658","4":"1.735487"},{"1":"0.775","2":"213","3":"84.64104","4":"2.459421"},{"1":"1.000","2":"188","3":"84.77260","4":"1.683740"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<pre class="r"><code>best_colsample &lt;- tune_randcol_best$colsample[1]</code></pre>
</div>
<div id="tune-learning-rate-pt.-2" class="section level2">
<h2>Tune learning rate (pt. 2)</h2>
<ul>
<li>use optimal tree parameters</li>
<li>use optimal number of trees</li>
<li>tune using lower learning rates</li>
</ul>
<pre class="r"><code>grid_learn &lt;- expand.grid(learn_rate = seq(.001, .15, length.out = 5))

tic()
fit_tune_learn2 &lt;- map(grid_learn$learn_rate, ~ {
 xgb.cv(
   data = features,
   label = outcome,
   nrounds = best_numtrees, # number of trees
   objective = &quot;reg:squarederror&quot;,
   early_stopping_rounds = 20, 
   nfold = 10,
   params = list(eta = .x,
                 max_depth = best_depth,
                 colsample_bytree = best_colsample),
   verbose = 0
 ) 
})
toc()</code></pre>
<pre><code>## 577.299 sec elapsed</code></pre>
<p>Below are the results of the tuned models:</p>
<pre class="r"><code>fit_tune_learn2_log &lt;- map_df(fit_tune_learn2, ~{
  .x$evaluation_log %&gt;% 
  pivot_longer(-iter,
               names_to = c(&quot;set&quot;,&quot;metric&quot;,&quot;measure&quot;),
               names_sep = &quot;_&quot;) %&gt;% 
  pivot_wider(names_from = &quot;measure&quot;,
              values_from = &quot;value&quot;)
}, .id = &quot;learn_rate&quot;) %&gt;% 
  mutate(learn_rate = factor(learn_rate,                              
                             labels = as.character(round(grid_learn$learn_rate,3)))) %&gt;% 
  filter(iter &gt; 10)

ggplot(fit_tune_learn2_log, aes(iter, mean)) +
  geom_line(aes(color = set), size = 1) +
  labs(x = &quot;Tree Iteration&quot;, y = &quot;Mean RMSE&quot;) +
  scale_color_manual(values = c(&quot;#FFDB6D&quot;, &quot;#00AFBB&quot;)) +
  theme_407() +
  facet_grid(~learn_rate) +
  theme(axis.text.x = element_text(size = 8))</code></pre>
<p><img src="page4_xgboost_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<pre class="r"><code>(tune_learn2_best &lt;- map_df(fit_tune_learn2, ~{
  .x$evaluation_log %&gt;% 
  dplyr::slice(.x$best_iteration)
}) %&gt;% 
  mutate(learning_rate = grid_learn$learn_rate) %&gt;% 
  arrange(test_rmse_mean) %&gt;% 
  select(learning_rate, iter, test_rmse_mean, test_rmse_std))</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":["learning_rate"],"name":[1],"type":["dbl"],"align":["right"]},{"label":["iter"],"name":[2],"type":["dbl"],"align":["right"]},{"label":["test_rmse_mean"],"name":[3],"type":["dbl"],"align":["right"]},{"label":["test_rmse_std"],"name":[4],"type":["dbl"],"align":["right"]}],"data":[{"1":"0.15000","2":"301","3":"84.40913","4":"1.885748"},{"1":"0.07550","2":"540","3":"84.44191","4":"1.337887"},{"1":"0.11275","2":"331","3":"84.50625","4":"1.829194"},{"1":"0.03825","2":"935","3":"84.54767","4":"2.893117"},{"1":"0.00100","2":"1000","3":"925.20374","4":"3.376594"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
<pre class="r"><code>best_learnrate2 &lt;- tune_learn2_best$learning_rate[1]</code></pre>
</div>
</div>
<div id="fit-the-final-model" class="section level1">
<h1>Fit the final model</h1>
<div id="finalize-model" class="section level2">
<h2>Finalize model</h2>
<p>The final model using the optimal hyperparameters was trained on the training set.</p>
<pre class="r"><code>tic()

fit_final_xgb &lt;- xgboost(
   data = features,
   label = outcome,
   nrounds = best_numtrees, # number of trees
   objective = &quot;reg:squarederror&quot;,
   early_stopping_rounds = 20, 
   params = list(eta = best_learnrate2,
                 max_depth = best_depth,
                 colsample_bytree = best_colsample),
   verbose = 0
 ) 

toc()</code></pre>
<pre><code>## 19.097 sec elapsed</code></pre>
</div>
<div id="preprocess-test-split" class="section level2">
<h2>Preprocess test split</h2>
<p>The testing set was preprocessed using the same recipe as the training data.</p>
<pre class="r"><code># Preprocess data
test_baked &lt;- rec %&gt;% 
  prep() %&gt;% 
  bake(test)

# Transform tst_dt to numeric
test_baked$tst_dt &lt;- as.numeric(test_baked$tst_dt)

# Transform preprocessed data into feature matrix
test_data &lt;- test_baked %&gt;% 
  select(-score, -contains(&quot;id&quot;), -ncessch, -sch_name) %&gt;% 
  as.matrix()

# Saving the scores as a separate vector
test_outcome &lt;- test_baked$score</code></pre>
</div>
<div id="test-final-model" class="section level2">
<h2>Test final model</h2>
<p>The final model was used to make predictions in the testing set. The performance of the model was evaluated using RMSE.</p>
<pre class="r"><code># Make predictions
predictions &lt;- predict(fit_final_xgb, test_data)

# Calculate RMSE
pred_tbl &lt;- tibble(predictions, test_outcome)
(test_rmse &lt;- yardstick::rmse(pred_tbl, predictions, test_outcome))</code></pre>
<div data-pagedtable="false">
<script data-pagedtable-source type="application/json">
{"columns":[{"label":[".metric"],"name":[1],"type":["chr"],"align":["left"]},{"label":[".estimator"],"name":[2],"type":["chr"],"align":["left"]},{"label":[".estimate"],"name":[3],"type":["dbl"],"align":["right"]}],"data":[{"1":"rmse","2":"standard","3":"85.75623"}],"options":{"columns":{"min":{},"max":[10]},"rows":{"min":[10],"max":[10]},"pages":{}}}
  </script>
</div>
</div>
</div>
<div id="final-thoughts" class="section level1">
<h1>Final thoughts</h1>
<p>The final RMSE on the assessment set was relatively good at 85.76. However, this isn’t far off from the mean RMSE when cross-validating a boosted tree model using the default parameters, 85.34. This finding suggests tuning did not add any predictive power to our model. Given that each tuning step took about 3 - 10 minutes, the change in RMSE does not seem worth the additional effort. There are a number of other hyperparameters that can be tuned in boosted tree models, including minimum n for each node in a tree or thresholding the change in cost function before stopping a tree. It is possible these additional parameters would have improved the performance of our model. Overall, though the boosted tree model demonstrated low variance and had relatively good predictive power, it was computationally inefficient in terms of how many hyperparameters can be tuned and how long it takes to tune, even with just 10% of the data.</p>
<hr />
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->
<script>
$(document).ready(function () {
  window.initializeCodeFolding("hide" === "show");
});
</script>

<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
