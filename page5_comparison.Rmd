---
title: "Tensor Flo Ridas: A Journey in Machine Learning"
output: 
  html_document:
    toc: TRUE
    toc_depth: 1
    toc_float: TRUE
    code_folding: hide
---

```{r load_libraries, include=FALSE, message=FALSE}
library(here)
library(tidyverse)
library(tidymodels)
library(rio)
```

***

# Model Comparisons

For this project, we aimed to predict student testing scores for grades 3-8. We chose to build three different models:

1. Linear regression
2. Random forest
3. Boosted trees

The final fits for each of the models are listed in the table below:

```{r}
df <- tibble(model = c("linear regression", "random forest", "boosted tree"),
             rmse = c(87, 87.4, 85.5))
print(df)
```

The simplest model and the least computationally demanding was the linear regression, which resulted in the second best RMSE. The random forest, which required tuning for three hyperparemeters (minimum n per node, number of columns to sample, and levels??), was computationally demanding (final timing?) and had worse predictive power than the linear regression. The best fitting model was the boosted tree, with an RMSE of `r df$rmse[df$model=="boosted tree"]`. Though the boosted tree was the best performing model, it was the most computationally demanding. Tuning just 4 hyperparameters (out of the many more possible hyperparameters) took over 40 minutes to complete and did not result in significant reduction in RMSE compared to the default model. 

It is important to note the data set used for the random forest and boosted tree models was only 10% of the total observations, while the data set for the linear model included all observations. Sampling of the entire data set was stratified by the outcome measure to reduce model variance, however, it is possible that sampled the observations still limited the performance of the random forest and boosted tree models. If we were to fit a final boosted tree or random forest model to the entire training data, we may find that our final predictions (i.e. on test.csv) are considerably more accurate. 

Taking all things into consideration, we decided to use XX for our final model. 

# Generating Final Predictions

## Preparing the final test data

```{r import_data, message=FALSE}
# Import joined data file
full_train <- import(here("data","data.csv"),
               setclass = "tbl_df")

# Import joined test file
final_test <- import(here("data","test_joined.csv"))
```

```{r define_recipe}
rec <- recipe(score ~ ., full_train) %>%  
    step_mutate(tst_dt = lubridate::mdy_hm(tst_dt)) %>%
    update_role(contains("id"), ncessch, sch_name, new_role = "id vars") %>%
    step_novel(all_nominal()) %>%
    step_unknown(all_nominal()) %>%
    step_zv(all_predictors()) %>%
    step_normalize(all_numeric(), -all_outcomes(), -has_role("id vars")) %>%
    step_BoxCox(all_numeric(), -all_outcomes(), -has_role("id vars")) %>%
    step_medianimpute(all_numeric(), -all_outcomes(), -has_role("id vars")) %>%
    step_dummy(all_nominal(), -has_role("id vars"), one_hot = TRUE) %>%
    step_zv(all_predictors())

rec2 <- recipe(score ~ ., full_train) %>%  
    step_mutate(tst_dt = lubridate::mdy_hms(tst_dt)) %>%
    update_role(contains("id"), ncessch, sch_name, new_role = "id vars") %>%
    step_novel(all_nominal()) %>%
    step_unknown(all_nominal()) %>%
    step_zv(all_predictors()) %>%
    step_normalize(all_numeric(), -all_outcomes(), -has_role("id vars")) %>%
    step_BoxCox(all_numeric(), -all_outcomes(), -has_role("id vars")) %>%
    step_medianimpute(all_numeric(), -all_outcomes(), -has_role("id vars")) %>%
    step_dummy(all_nominal(), -has_role("id vars"), one_hot = TRUE) %>%
    step_zv(all_predictors())
```

```{r preproc}
full_train_baked <- rec %>%
    prep() %>%
    bake(full_train)

# final_test_baked <- rec2 %>%
#     prep() %>%
#     bake(final_test)

```

## Final model fit

```{r final_model}

```


```{r final_predictions}
# # Fit our final model to the full training data
# full_fit <- fit(mod_1, 
#                 score ~ ., 
#                 data = select(full_train_baked, -contains("id"), -ncessch, -sch_name))
# 
# #prediction
# preds <- predict(full_fit, 
#                  new_data = final_test_baked)
# preds
# 
# #write out predictions 
# pred_frame <- tibble(Id = final_test_baked$id, Predicted = preds$.pred)
# head(pred_frame)
# 
# #write out the file 
# write_csv(pred_frame, "data/final_fits.csv")

```



***
