---
title: "Tensor Flo Ridas: A Journey in ML"
output: 
  html_document:
    toc: TRUE
    toc_depth: 1
    toc_float: TRUE
    code_folding: show
    df_print: paged
---

```{r load_libraries, include = FALSE, message = FALSE}
# load required packages
library(tidyverse)
library(tidymodels)
library(here)
library(rio)
library(workflows)
library(vip)
library(magrittr)

# create theme
theme_407 <- function() { 
    theme_bw(base_size = 5) %+replace% 
        theme(
          # legend.direction = "horizontal",
          legend.background = element_rect(fill = "#1b1d22",
                                           colour = "#1b1d22"),
          panel.background = element_rect(fill = "#171f24",
                              colour = "#1b1d22",
                              size = 0.5, linetype = "solid"),
          panel.grid.major = element_line(size = 0.2, 
                                          linetype = 'solid', 
                                          colour = "gray40"), 
          panel.grid.minor = element_line(size = 0.0, 
                                          linetype = 'solid',
                                          colour = "gray40"),
          axis.line = element_line(colour = "white"),
          plot.background = element_rect(fill = "#171f24"),
          text  = element_text(color = "white", size = 10),
          axis.text  = element_text(color = "white"),
          axis.ticks = element_line(color = "white"),
          strip.background = element_rect(fill = "white",
                                          colour = "white")
        )
}


# import all data
data <- import(here::here("data", "data.csv"), setclass = "tibble") %>%
  janitor::clean_names()

set.seed(3000)

# split data
data_split    <- initial_split(data, strata = "score")

data_train    <- training(data_split)

data_test     <- testing(data_split)

data_train_cv <- vfold_cv(data_train)

# create recipe
recipe_1 <- recipe(score ~ ., data_train) %>%  
    step_mutate(tst_dt = lubridate::mdy_hm(tst_dt)) %>%
    update_role(contains("id"), ncessch, sch_name, new_role = "id vars") %>%
    step_novel(all_nominal()) %>%
    step_unknown(all_nominal()) %>%
    step_zv(all_predictors()) %>%
    step_normalize(all_numeric(), -all_outcomes(), -has_role("id vars")) %>%
    step_BoxCox(all_numeric(), -all_outcomes(), -has_role("id vars")) %>%
    step_medianimpute(all_numeric(), -all_outcomes(), -has_role("id vars")) %>%
    step_dummy(all_nominal(), -has_role("id vars"), one_hot = TRUE) %>%
    step_zv(all_predictors())

```

***

# Linear Model 

## Introduction 

A brief recap, our project aims to predict student math and reading test score for 3rd to 8th grade students in Oregon using data from the National Center for Educational Statistics. 

To obtain a general understanding of how our predictor variables fit our data, we begin by fitting a linear regression model to our data akin to `stats::lm`. Our criteria for evaluation of model fit will be the root mean square error (RMSE). The result of the linear model will be used to inform subsequent model selection and tuning. 

We will use the following recipe (for more information, [see our Data section](page1_data.html)) to fit our model. 


```{r recipe} 
rec <- recipe(score ~ ., data_train) %>%  
    step_mutate(tst_dt = lubridate::mdy_hm(tst_dt)) %>%
    update_role(contains("id"), ncessch, sch_name, new_role = "id vars") %>%
    step_novel(all_nominal()) %>%
    step_unknown(all_nominal()) %>%
    step_zv(all_predictors()) %>%
    step_normalize(all_numeric(), -all_outcomes(), -has_role("id vars")) %>%
    step_BoxCox(all_numeric(), -all_outcomes(), -has_role("id vars")) %>%
    step_medianimpute(all_numeric(), -all_outcomes(), -has_role("id vars")) %>%
    step_dummy(all_nominal(), -has_role("id vars"), one_hot = TRUE) %>%
    step_zv(all_predictors())

rec
```

As shown in our recipe summary, our recipe contains 7 id variables (these variables will not be used in our prediction), 1 outcome (test score) and 45 predictors. 


## Build Linear Model 

To build our linear model, we will use the [`parsnip`](https://parsnip.tidymodels.org/) and [`workflows`](https://workflows.tidymodels.org/) package that is a part of the [`tidymodels`](https://www.tidymodels.org/) family of packages developed to facilitate modeling and machine learning. 

`parsnip` provides an unified interface of common machine learning algorithms where users simply have to specify the model and engine rather than having to remember model specifications that may differ across different algorithm packages. Here is a reference [library](https://www.tidymodels.org/find/parsnip/) of `parsnip` models. 

Once we have created our recipe and `parsnip` model, we can then use `workflows` to combine our recipe and model together to fit our 10-fold cross validation (CV) data. 

We specify our `parsnip` linear model by setting the engine as `lm` and mode as `regression` given our outcome variable (test score) is continous. 

```{r build_model}
#create model 
lin_mod <- linear_reg() %>% 
  set_engine("lm")  %>% 
  set_mode("regression")

lin_mod
```

We then use `add_recipe()` and `add_model()` to combine our recipe and linear model into a workflow. 

```{r workflow}
#create workflow
lin_workflow <- workflow() %>% 
  add_recipe(recipe_1) %>% 
  add_model(lin_mod)
```

## Fit Linear Model 

We now fit our 10-fold CV data to our linear model using `fit_resamples()` which fits a single model and recipe across mulitple resamples.  

```{r fit_model, message = FALSE}
#run model 
start_rf <- Sys.time()

lin_fit <- fit_resamples(
  lin_workflow,
  data_train_cv,
  metrics = yardstick::metric_set(rmse), 
  control = control_resamples(verbose = TRUE, 
                              save_pred = TRUE))
end_rf <- Sys.time()
(lin_time <- end_rf - start_rf)
```

Our linear model took `r round(lin_time, 2)` minutes to run. 

## Collect Metrics 

After sucessfully running our linear model, we collect our RMSE to evaluate model fit. Listed below are the RMSE for each individual fold across our 10-fold CV along with the average RMSE across the 10-fold. 

```{r}
# see all RMSE
lin_fit %>% 
  collect_metrics(summarize = FALSE)

# see average RMSE 
lin_fit %>%
  collect_metrics()
```

```{r, include = FALSE}
train_rmse <- lin_fit %>% 
  collect_metrics(summarize = TRUE) 
```

The average RMSE for our linear model on the 10-fold CV was `r round(train_rmse$mean,2)`

# Make Prediction

To evaluate how well our trained linear model is able to predict un-trained or un-seen data, we will now make predictions on our left out dataset from our initial split. 

```{r, message = FALSE}
#run the model on the withheld test set from the initial split 
start_rf <- Sys.time()

mod_lin_final <- last_fit(
lin_workflow, 
split = data_split)

end_rf <- Sys.time()
(lin_final_time <- end_rf - start_rf)
```

Applying the linear model to our left out dataset from our initial split was much faster (took only `r round(lin_final_time, 2)` seconds to run!) compared to our 10-fold cv training data (`r round(lin_time, 2)` min) where the model was fitted across multiple folds. 

## Collect Metrics 
```{r}
mod_lin_final %>% 
  collect_metrics()
```


```{r, include = FALSE}
holdout_rmse <- mod_lin_final %>% collect_metrics(summarize = TRUE) %>% filter(.metric == "rmse")
holdout_rsq <- mod_lin_final %>% collect_metrics(summarize = TRUE) %>% filter(.metric == "rsq")
```

The average RMSE for our linear model on the withheld test data was `r round(holdout_rmse$.estimate,2)`. This is a very slight improvement in RMSE compared to the RMSE of `r round(train_rmse$mean,2)` obtained in our 10-fold cv training data. 

# Plot - Important Predictors 

We can plot the most important predictors in our linear model using the `vip` package.  

```{r class.source = 'fold-hide'}
mod_lin_final %>% 
  pluck(".workflow", 1) %>%   
  pull_workflow_fit() %>% 
  vip(num_features = 20, 
      aesthetics = list(fill = "cyan3")) +
  labs(y = "Variable Importance", 
       title = "Variable Importance Plot for Linear Model") +
  theme_407() +
  theme(plot.title.position = "plot") 
```

From the plot, we can see that the variable `sp_ed_fg_N` which indicates whether a student participates in an Individualized Education Plan was one of the most important predictor of students' test score. Other important predictors include students' Limited English Proficiency status and ethnicity. 

# Discussion

The linear model provided an introduction to machine learning and a general estimate of the RMSE model fit based on all of our predictor variables. Using the linear model, our predictors were able to account for `r round(holdout_rsq$.estimate*100, 2)`% of the variance in our score with an RMSE of `r round(holdout_rmse$.estimate, 2)` in our left-out dataset. 

Overall, the linear model was extremely quick to run computationally (only took `r round(lin_time, 2)` to run through our 10-fold CV) as the algorithm makes a lot of assumptions about the data (i.e., linear model has a high bias) and there were no hyperparameter tuning involved in the linear model (we will briefly touch upon penalized regression which allows for tuning in the seciton below). Given that the linear model makes high assumption of the data, the linear model will have relatively low variance when applied to new unseen data. However, given the high-bias and low-variance of the linear model, we will generally expect the model to produce consistent prediction that may not be the most accurate. 

To improve our RMSE, we will explore application of non-linear models - [Random Forest](page3_rf.html) and [Boosted Tree](page4_xgboost.html) - to our data that can also be tuned for an optimal bias-variance balance. 


## Penalized Regression 

As mentioned above, linear model makes high assumptions about the data and does not provide any hyperparameters for tuning bias-variance balance. One can look into penalized regression (also known as regularized regression or shrinkage method) which introduces bias 

***

```{r, echo = FALSE}
today <- Sys.Date() 
```

<center>
*Last updated: `r format(today, format="%B %d %Y")`.*
</center>